

简单学习了flask和爬虫后、用一个小例子练练手
因为比较懒，就直接用flask_mongoengine作为ORM了。

## 爬取豆瓣top250电影列表存储到mongo
```py
import requests
import datetime
from flask import Flask
from bs4 import BeautifulSoup
from flask_mongoengine import MongoEngine

app = Flask(__name__)
app.config['MONGODB_SETTINGS'] = {
    'db': 'tiktok',
    'host': 'localhost',
    'port': 27017
}
db = MongoEngine()
db.init_app(app)


class Movie(db.Document):
    name = db.StringField() # 电影名
    poster = db.StringField() # 海报
    ranking = db.StringField(default='') # 排名
    grade = db.StringField(default='') # 评分
    author = db.StringField(default='') # 导演、主演 。。。
    introduction = db.StringField(default='') # 简介
    created_at = db.DateTimeField(default=datetime.datetime.utcnow)


def request_douban(url):
    try:
        response = requests.get(
            url,
            headers={
                'User-Agent':
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
            })
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


def save_to_db(soup):
    list = soup.find(class_='grid_view').find_all('li')

    for item in list:
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        if (item.find(class_='inq') != None):
            item_intr = item.find(class_='inq').string

        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score +
              ' | ' + item_intr)

        movie = Movie(name=item_name,
                      poster=item_img,
                      ranking=item_index,
                      grade=item_score,
                      author=item_author.strip(),
                      introduction=item_intr)
        movie.save()


def getMovie(page):
    url = 'https://movie.douban.com/top250?start=' + str(
        page * 25) + '&filter='
    html = request_douban(url)
    if html is None:
        print('未获取到内容')
    else:
        soup = BeautifulSoup(html, 'lxml')
        save_to_db(soup)


if __name__ == '__main__':
    # app.debug = True
    # app.run()
    for i in range(0, 1):  # 如果要爬取完整250个 就设置为(0,10)
        getMovie(i)

```
